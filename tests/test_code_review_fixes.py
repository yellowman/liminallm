"""Tests for code review fixes - edge cases identified during comprehensive analysis."""

from __future__ import annotations

import threading
from concurrent.futures import ThreadPoolExecutor
from unittest.mock import MagicMock, patch

import pytest

from liminallm.api.schemas import ErrorBody, Envelope
from liminallm.config import AdapterMode


# ==============================================================================
# Zero-Weight Adapter Handling Tests
# ==============================================================================


class TestZeroWeightAdapterHandling:
    """Test that weight=0.0 is handled correctly (not treated as falsy)."""

    def test_zero_weight_not_replaced(self):
        """weight=0.0 should NOT be replaced with default 1.0."""
        from liminallm.service.model_backend import ApiAdapterBackend

        backend = ApiAdapterBackend("meta-llama", provider="together")

        # Adapter with explicit weight=0.0 should keep that weight
        adapters = [
            {"id": "disabled", "remote_adapter_id": "lora-disabled", "weight": 0.0},
            {"id": "enabled", "remote_adapter_id": "lora-enabled", "weight": 0.8},
        ]

        # Use _select_best_adapter which relies on weight extraction
        result = backend._select_best_adapter(adapters, max_count=2)

        # Enabled (0.8) should come first, disabled (0.0) second
        assert result[0]["id"] == "enabled"
        assert result[1]["id"] == "disabled"

    def test_zero_weight_in_format_remote_adapters(self):
        """weight=0.0 should be passed correctly to gate weights."""
        from liminallm.service.model_backend import ApiAdapterBackend

        backend = ApiAdapterBackend("meta-llama", provider="together")

        adapters = [
            {"id": "a1", "remote_adapter_id": "lora-1", "weight": 0.0},
        ]

        model, extra_body, applied, dropped = backend._format_remote_adapters(adapters)

        # Extra body should contain the gate weight
        if extra_body and "adapter_weights" in extra_body:
            weights = extra_body["adapter_weights"]
            if isinstance(weights, list):
                assert 0.0 in weights
            else:
                assert weights == 0.0

    def test_none_weight_defaults_to_one(self):
        """None weight should default to 1.0 (not 0.0)."""
        from liminallm.service.model_backend import ApiAdapterBackend

        backend = ApiAdapterBackend("meta-llama", provider="together")

        adapters = [
            {"id": "no_weight", "remote_adapter_id": "lora-1"},  # No weight field
            {"id": "explicit", "remote_adapter_id": "lora-2", "weight": 0.5},
        ]

        result = backend._select_best_adapter(adapters, max_count=2)

        # no_weight (default 1.0) should come first
        assert result[0]["id"] == "no_weight"


# ==============================================================================
# Thread-Safe Memory Store Counter Tests
# ==============================================================================


class TestMemoryStoreThreadSafety:
    """Test thread safety of memory store sequence counters."""

    def test_artifact_version_seq_thread_safe(self):
        """_next_artifact_version_id should be thread-safe."""
        from liminallm.storage.memory import MemoryStore
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            store = MemoryStore(fs_root=tmpdir)

            # Collect IDs from multiple threads
            collected_ids = []
            lock = threading.Lock()

            def get_next_id():
                for _ in range(100):
                    next_id = store._next_artifact_version_id()
                    with lock:
                        collected_ids.append(next_id)

            threads = [threading.Thread(target=get_next_id) for _ in range(10)]
            for t in threads:
                t.start()
            for t in threads:
                t.join()

            # All IDs should be unique (no duplicates)
            assert len(collected_ids) == len(set(collected_ids))
            assert len(collected_ids) == 1000

    def test_chunk_id_seq_thread_safe(self):
        """_next_chunk_id should be thread-safe."""
        from liminallm.storage.memory import MemoryStore
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            store = MemoryStore(fs_root=tmpdir)

            collected_ids = []
            lock = threading.Lock()

            def get_next_id():
                for _ in range(100):
                    next_id = store._next_chunk_id()
                    with lock:
                        collected_ids.append(next_id)

            threads = [threading.Thread(target=get_next_id) for _ in range(10)]
            for t in threads:
                t.start()
            for t in threads:
                t.join()

            # All IDs should be unique
            assert len(collected_ids) == len(set(collected_ids))


# ==============================================================================
# Token-Based RAG Chunking Tests
# ==============================================================================


class TestTokenBasedRAGChunking:
    """Test token-based chunking with overlap per SPEC ยง2.5."""

    def test_simple_tokenize(self):
        """_simple_tokenize should split text into word tokens."""
        from liminallm.service.rag import _simple_tokenize

        tokens = _simple_tokenize("Hello, world! How are you?")

        assert "Hello" in tokens
        assert "world" in tokens
        assert "," in tokens
        assert "?" in tokens

    def test_detokenize(self):
        """_detokenize should reconstruct text from tokens."""
        from liminallm.service.rag import _detokenize

        tokens = ["Hello", ",", "world", "!"]
        result = _detokenize(tokens)

        assert "Hello" in result
        assert "world" in result

    def test_chunking_creates_overlap(self):
        """Chunks should have overlapping tokens."""
        from liminallm.service.rag import RAGService, _simple_tokenize
        from liminallm.storage.memory import MemoryStore
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            store = MemoryStore(fs_root=tmpdir)
            rag = RAGService(store, default_chunk_size=50, rag_mode="memory")

            # Create a test context
            user = store.create_user("test@test.com", tenant_id="test")
            ctx = store.upsert_context(
                name="test",
                description="test context",
                owner_user_id=user.id,
            )

            # Ingest long text
            long_text = " ".join([f"word{i}" for i in range(200)])
            chunk_count = rag.ingest_text(ctx.id, long_text, chunk_size=50, overlap_tokens=20)

            assert chunk_count > 1

            # Verify chunks were created with metadata
            chunks = store.list_chunks(ctx.id)
            assert len(chunks) > 1

            # Check that overlap metadata exists
            for i, chunk in enumerate(chunks):
                if chunk.meta and i > 0:
                    assert "overlap_tokens" in chunk.meta

    def test_chunk_metadata_includes_token_info(self):
        """Chunks should have token count metadata."""
        from liminallm.service.rag import RAGService
        from liminallm.storage.memory import MemoryStore
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            store = MemoryStore(fs_root=tmpdir)
            rag = RAGService(store, default_chunk_size=100, rag_mode="memory")

            # Create context
            user = store.create_user("test@test.com")
            ctx = store.upsert_context(
                name="test",
                description="test",
                owner_user_id=user.id,
            )

            text = "This is a test text with multiple words for chunking."
            rag.ingest_text(ctx.id, text, chunk_size=100)

            chunks = store.list_chunks(ctx.id)
            assert len(chunks) >= 1

            for chunk in chunks:
                if chunk.meta:
                    assert "token_count" in chunk.meta
                    assert "embedding_model_id" in chunk.meta


# ==============================================================================
# ErrorBody Code Validation Tests
# ==============================================================================


class TestErrorBodyValidation:
    """Test ErrorBody.code validation per SPEC ยง18."""

    def test_valid_error_codes(self):
        """Valid SPEC ยง18 error codes should be accepted."""
        valid_codes = [
            "unauthorized",
            "forbidden",
            "not_found",
            "rate_limited",
            "validation_error",
            "conflict",
            "server_error",
        ]

        for code in valid_codes:
            error = ErrorBody(code=code, message="test")
            assert error.code == code

    def test_invalid_error_code_rejected(self):
        """Invalid error codes should be rejected."""
        with pytest.raises(ValueError) as exc_info:
            ErrorBody(code="invalid_code", message="test")

        assert "Invalid error code" in str(exc_info.value)

    def test_envelope_with_valid_error(self):
        """Envelope should accept valid ErrorBody."""
        error = ErrorBody(code="not_found", message="Resource not found")
        envelope = Envelope(status="error", error=error)

        assert envelope.status == "error"
        assert envelope.error.code == "not_found"


# ==============================================================================
# Training Loss Recording Tests
# ==============================================================================


class TestTrainingLossRecording:
    """Test that actual training loss is recorded, not heuristic."""

    def test_extracts_loss_from_training_trace(self):
        """Should extract actual loss from JAX training trace."""
        # Mock the training trace returned by _run_jax_optax_training
        training_trace = {
            "status": "ok",
            "steps": [
                {"step": 0, "loss": 2.5},
                {"step": 1, "loss": 1.8},
                {"step": 2, "loss": 0.9},  # Final loss
            ],
            "final_params_path": "/tmp/params.json",
        }

        # The logic from training.py
        loss = 1.0 / (1 + 10)  # Heuristic fallback
        if training_trace.get("status") == "ok" and training_trace.get("steps"):
            final_step = training_trace["steps"][-1]
            if isinstance(final_step, dict) and "loss" in final_step:
                actual_loss = final_step.get("loss")
                if isinstance(actual_loss, (int, float)) and actual_loss >= 0:
                    loss = float(actual_loss)

        # Should use actual loss from trace, not heuristic
        assert loss == 0.9

    def test_falls_back_to_heuristic_if_no_trace(self):
        """Should fall back to heuristic if training trace is empty."""
        training_trace = {"status": "error", "steps": []}

        dataset_entries = list(range(10))
        loss = 1.0 / (1 + len(dataset_entries))  # Heuristic

        if training_trace.get("status") == "ok" and training_trace.get("steps"):
            final_step = training_trace["steps"][-1]
            if isinstance(final_step, dict) and "loss" in final_step:
                actual_loss = final_step.get("loss")
                if isinstance(actual_loss, (int, float)) and actual_loss >= 0:
                    loss = float(actual_loss)

        # Should use heuristic since trace status is error
        assert loss == pytest.approx(1.0 / 11)


# ==============================================================================
# ChatMessage.mode Validation Tests
# ==============================================================================


class TestChatMessageModeValidation:
    """Test that ChatMessage.mode should be validated."""

    def test_valid_modes(self):
        """Valid modes should be accepted."""
        from liminallm.api.schemas import ChatMessage

        # text mode
        msg_text = ChatMessage(content="hello", mode="text")
        assert msg_text.mode == "text"

        # voice mode
        msg_voice = ChatMessage(content="base64audio", mode="voice")
        assert msg_voice.mode == "voice"

    def test_default_mode_is_text(self):
        """Default mode should be text."""
        from liminallm.api.schemas import ChatMessage

        msg = ChatMessage(content="hello")
        assert msg.mode == "text"


# ==============================================================================
# Refresh Token Revocation Security Tests
# ==============================================================================


class TestRefreshTokenRevocationSecurity:
    """Test that refresh token revocation defaults to safe behavior."""

    @pytest.mark.asyncio
    async def test_cache_failure_defaults_to_revoked(self):
        """When Redis cache fails, should assume token is revoked (safe default)."""
        from liminallm.service.auth import AuthService
        from liminallm.config import Settings
        from unittest.mock import AsyncMock

        # Create auth service with failing cache
        mock_store = MagicMock()
        mock_cache = MagicMock()
        mock_cache.is_refresh_revoked = AsyncMock(side_effect=Exception("Redis unavailable"))
        mock_settings = MagicMock(spec=Settings)
        mock_settings.jwt_secret = "test-secret-key-at-least-32-chars"
        mock_settings.jwt_algorithm = "HS256"

        auth = AuthService(store=mock_store, cache=mock_cache, settings=mock_settings)
        auth.revoked_refresh_tokens = set()  # Empty local set

        # Should return True (revoked) when cache fails
        result = await auth._is_refresh_revoked("test-jti")
        assert result is True  # Safe default: assume revoked

    @pytest.mark.asyncio
    async def test_cache_success_returns_actual_value(self):
        """When cache succeeds, should return actual revocation status."""
        from liminallm.service.auth import AuthService
        from liminallm.config import Settings
        from unittest.mock import AsyncMock

        mock_store = MagicMock()
        mock_cache = MagicMock()
        mock_cache.is_refresh_revoked = AsyncMock(return_value=False)
        mock_settings = MagicMock(spec=Settings)
        mock_settings.jwt_secret = "test-secret-key-at-least-32-chars"
        mock_settings.jwt_algorithm = "HS256"

        auth = AuthService(store=mock_store, cache=mock_cache, settings=mock_settings)
        auth.revoked_refresh_tokens = set()

        result = await auth._is_refresh_revoked("test-jti")
        assert result is False  # Token is not revoked


# ==============================================================================
# User ID Required in pgvector Search Tests
# ==============================================================================


class TestPgvectorUserIdRequired:
    """Test that user_id is required for pgvector search (data isolation)."""

    def test_empty_user_id_returns_empty(self):
        """Empty user_id should return empty results for safety."""
        # This tests the defense-in-depth check added to postgres.py
        # The actual method now requires user_id as a non-optional parameter

        # Mock the behavior we expect
        def search_with_empty_user_id(user_id):
            if not user_id:
                return []  # Defense in depth
            return ["results"]

        assert search_with_empty_user_id("") == []
        assert search_with_empty_user_id(None) == []
        assert search_with_empty_user_id("valid-user") == ["results"]


# ==============================================================================
# Pagination Query Parameter Validation Tests
# ==============================================================================


class TestPaginationValidation:
    """Test pagination parameter validation."""

    def test_page_must_be_positive(self):
        """page parameter must be >= 1."""
        from pydantic import BaseModel, Field, ValidationError

        class PaginationParams(BaseModel):
            page: int = Field(1, ge=1, description="Page number")

        # Valid page
        params = PaginationParams(page=1)
        assert params.page == 1

        # Invalid page should raise
        with pytest.raises(ValidationError):
            PaginationParams(page=0)

        with pytest.raises(ValidationError):
            PaginationParams(page=-1)

    def test_page_size_must_be_bounded(self):
        """page_size parameter must be 1-200."""
        from pydantic import BaseModel, Field, ValidationError

        class PaginationParams(BaseModel):
            page_size: int = Field(50, ge=1, le=200, description="Items per page")

        # Valid page_size
        params = PaginationParams(page_size=50)
        assert params.page_size == 50

        # Too small
        with pytest.raises(ValidationError):
            PaginationParams(page_size=0)

        # Too large
        with pytest.raises(ValidationError):
            PaginationParams(page_size=201)
